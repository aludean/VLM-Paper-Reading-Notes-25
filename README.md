综述
[ ] 【2509】【综述：RL MLLM】GitHub - Awesome-RL-based-Reasoning-MLLMs
[ ] 【2509】【综述：图像思考】Github - Awesome-Think-With-Images
[ ] 【2509】【综述：RL4LRM】A Survey of Reinforcement Learning for Large Reasoning Models【Code】
[X] 【2509】【综述：视觉压缩】https://github.com/daixiangzi/Awesome-Token-Compress/
[X] 【2509】【AI Lab】A Survey of Reinforcement Learning for Large Reasoning Models【Abstract】
[ ] 【2508】【综述：视觉 RL】Github-Awesome-Visual-Reinforcement-Learning
[ ] 【2508】【综述：VLM 视频】A Survey on Video Temporal Grounding with Multimodal Large Language Model
[ ] 【2505】【综述】Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models
[ ] 【2504】【综述：VLM RL】Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models
[ ] 【2503】【综述】Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning【Code】
[X] 【以往的月】arxiv多模态大模型论文推送

Company Technical Reports（大厂报告）
Qwen/Alibaba/Ant
[ ] 【2509】【Qwen3-VL】【Blog】【Code】
[ ] 【2509】【Qwen3-Omni】Qwen3-Omni Technical Report【Code】
[X] 【2508】【基于评分标准的奖励】Reinforcement Learning with Rubric Anchors【更好的验证答案】
[ ] 【2508】【qwen-dianjin】【Code】【金融 OCR】
[ ] 【2508】【M3PO】M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following
[ ] 【2508】【Ovis2.5】Ovis2.5 Technical Report【RLVR 数据：① 选择改填空 ② 图片嵌入词表】
[ ] 【2506】【Ovis-U1】Ovis-U1 Technical Report【Abstract】【Code】
[ ] 【2409】【mPLUG-DocOwl2】mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding
[ ] 【2408】【mPLUG-Owl3】mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models
Tencent
[X] 【2509】【PointsReader】POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion【Abstract】【OCR VLM 模型；① 文本渲染 HTML 合成数据 ② 迁移真实 OCR 场景】
Kwai
[ ] 【2509】【Keye-VL 1.5】Kwai Keye-VL 1.5 Technical Report
[ ] 【2508】【Thyme-RL】Thyme: Think Beyond Images【Code】【Data】【图像思考】
[ ] 【2507】【Keye-VL】Kwai Keye-VL Technical Report【Code】
[ ] 【2502】【Task Galaxy】TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types【Code】【文档】【数据集标注 pipeline】
Seed/Bytedance/Douyin
[ ] 【2509】【SAIL-VL2】SAIL-VL2 Technical Report
[ ] 【2508】【VeOmni】VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo【Abstract】【全模态训练框架】
[X] 【2508】【StructVRM】StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models【部分正确性过程 RLVR 奖励】
[ ] 【2507】【POLARIS】seed 强化学习训练经验-论文【Abstract/Abstract2】【Blog】【Code】【Datasets】【Models】
InternVL
[ ] 【2509】【InternVLA-A1】coming【Abstract】【Code】【Dataset】
[ ] 【2508】【InternVL3.5】InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency
[ ] 【2508】【InternVL-S1】Intern-S1: A Scientific Multimodal Foundation Model【Code】【Models】【精确解析，物理化学等科学任务】
[ ] 【2411】【InternVL2-MPO】Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization 【Blog】【Model】【Code】
Deepseek
[X] 【2510】【Deepseek-OCR】DeepSeek-OCR: Contexts Optical Compression【Abstract】【Code】【① 二维视觉压缩一维文字 ② 视觉遗忘机制 ③ 模态无关的推理核心】
[ ] 【2509】【Deepseek V3.2-Exp：DSA】【Abstract】
[ ] 【2502】【Deepseek：Native Sparse Attention】Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention【Abstract】【压缩 token，有限窗口注意】
[ ] 【2502】【Janus】
Kimi
[ ] 【2508】【OpenCUA】OpenCUA: Open Foundations for Computer-Use Agents【Abstract】【计算机 Agent：港大和kimi的opencua，模型、数据、标注平台、bench等都开源了】
[ ] 【2506】【Kimi-Researcher】Kimi-Researcher Technical Report【Abstract】
[ ] 【2504】【Kimi-VL】Kimi-VL Technical Report 【Blog】【Models】【Demo】【Code】
[ ] 【2501】【Kimi k1.5】Kimi k1.5: Scaling Reinforcement Learning with LLMs 【Blog】
Meituan
[ ] 【2509】【LongCat-Flash-Thinking】LongCat-Flash-Thinking Technical Report【多个模型单独训练】
GLM
[ ] 【2508】【GLM-4.5V】GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning【Abstract】
[ ] 【2507】【GLM-4.5】GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models【Blog】【Code】【Models】
[ ] 【2507】【GLM-4.1V-Thinking】GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning【Code】
Gemini
[ ] 【2508】【Genie3】【Abstract】
[ ] 【2507】【Gemini 2.5Pro & Flash】Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
[ ] 【2506】【Gemma 3n】Gemma 3 Technical Report【Abstract】
Baidu
[ ] 【2506】【ERNIE 4.5】Announcing the Open Source Release of the ERNIE 4.5 Model Family【Code】
Rednote
[ ] 【2508】【dots.vlm1】【Code】
[ ] 【2506】【dots.llm1】dots.llm1 Technical Report【Code】
[ ] 【2503】【小红书：奖励模型虚假关联】The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models【Abstract】
Xiaomi
[ ] 【2508】【Mimo-VL】MiMo-VL Technical Report【Abstract/Abstract2】【Code】【Model】
[ ] 【2504】【Mimo】MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining【Code】
Minimax
[ ] 【2506】【MiniMax-m1】MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention【Code】
Step 阶跃星辰
[ ] 【2507】【Step3】Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding【Abstract】
Skywork
Text
[ ] 【2505】【skyworkor1】Skywork Open Reasoner 1 Technical Report【Abstract】【Code】【Datasets】【Models】【熵坍缩，详细的消融实验】
MultiModal
[ ] 【2507】【Skywork R1V3】Skywork-R1V3 Technical Report【Codes】【① 关键推理 token 的熵，②connector 模态对齐】
[ ] 【2504】【Skywork R1V2】Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning 【Models】【Code】【MPO 和 GRPO 混合推理】
[ ] 【2504】【Skywork R1V1】Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought
MiniCPM-V
[X] 【2508】【MiniCPM-V4.5】MiniCPM-V4.5 Technical Reports【Abstract】【Code】
LLaVA
[ ] 【2509】【LLaVA-OneVision-1.5】LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training
架构改进
推理压缩
[ ] 【2505】【Think Silently, Think Fast】Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains【latent token 压缩思维链长度】
视觉压缩
[ ] 【2406】【DeepStack】DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs【Abstract/Abstract2】
Textual/Multimodal Reward Model （奖励模型）
[ ] 【2509】【BaseReward】BaseReward: A Strong Baseline for Multimodal Reward Model【大量的 trick】
[ ] 【2509】【RewardDance】RewardDance: Reward Scaling in Visual Generation【Abstract】
[X] 【2508】【StructVRM】StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models【部分正确性过程 RLVR 奖励】
[ ] 【2507】【checklist】Checklists Are Better Than Reward Models For Aligning Language Models【清单比奖励模型更适合对齐语言模型】
[ ] 【2507】【GLM-4.1V-Thinking VLM Reward System】GLM-4.1V-Thinking VLM Reward System
[ ] 【2507】【POLAR】Pre-Trained Policy Discriminators are General Reward Models【Models】【Code】
[ ] 【2505】【Skywork-VL Reward】Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning 【Models】【Code】
[ ] 【2505】【UnifiedReward-Think】Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning 【Blog】【Models】【Datasets】【Code】
[ ] 【2505】【R1-Reward】R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning 【Model】【Dataset】【Code】
[ ] 【2505】【WorldPM】WorldPM: Scaling Human Preference Modeling【Model】【Code】
[ ] 【2504】【DeepSeek-GRM】Inference-Time Scaling for Generalist Reward Modeling【Abstract】
[ ] 【2503】【小红书：奖励模型虚假关联】The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models【Abstract】
[ ] 【2503】【VisualPRM】VisualPRM: An Effective Process Reward Model for Multimodal Reasoning【Blog】【Models】【Datasets-Raw】【Datasets-Conversation】【Code】
[ ] 【2502】【MM-RLHF】MM-RLHF: The Next Step Forward in Multimodal LLM Alignment【Blog】【Models】【Datasets】【Code】【RewardBench】
[ ] 【2501】【InternLM-XComposer2.5-Reward】InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model【Abstract】【Code】
Text（纯文本）
[ ] 【2509】开源RL框架Verlog来了，专为LLM智能体打造，400回合不成问题
[X] 【2507】【Beyond Binary Rewards】Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty【Abstract】【RLVR 训练经验：正确+诚实，置信度 Reward】
Image（单图）
RLVR 训练
[ ] 【2510】【Rex-Omni】Detect Anything via Next Point Prediction【HF】【目标检测任务 ①使用特殊标记来表示从 0 到 999 的量化坐标 ② 数据 pipeline】
[X] 【2508】【Pass@K】Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models【Pass@K 比 Pass@1 更加鲁棒】
[X] 【2508】【FineVision：多模态数据集】【Abstract】
[X] 【2508】【Vision G1】Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation【大规模全面的 RLVR 数据集】
[X] 【2507】【Beyond Binary Rewards】Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty【正确+诚实，置信度 Reward】【Abstract】
[ ] 【2507】【POLARIS】seed 强化学习训练经验-论文 coming【Abstract】【Blog】【Code】【Datasets】【Models】
[ ] 【2507】【Open vision reasoner】Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning【Blog】【Code】【Models】【MathVision 指标 35，开源时的最高】
[X] 【2506】【No Free Lunch】No Free Lunch: Rethinking Internal Feedback for LLM Reasoning【内部熵奖励】
[X] 【2506】【OCR 遮挡】Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models【Abstract】【增强 OCR 识别遮挡能力，减少幻觉】
[ ] 【2506】【WeThink】WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning【Abstract】【Datasets】【Code】【RLVR 训练经验】
[ ] 【2506】【Revisual-R1】Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning 【Models】【Code】【Datasets】【先多模态后文本，推理能力迁移】
[ ] 【2506】【SynthRL】SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis 【Model】【Datasets】【Code】【Query 数据增强为更难的变体】
[ ] 【2505】【MoDoMoDo】MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning 【Blog】【Datasets】【Code】【大规模数据配比策略】
[ ] 【2505】【VisualSphinx】VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL 【Blog】【Model】【Datasets】【Code】【大规模数据合成；公务员行测题数据】
[ ] 【2505】【MM-UPT】Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO 【Model】【Dataset】【Code】【RLVR 训练经验】
[ ] 【2505】【Orsta】One RL to See Them All: Visual Triple Unified Reinforcement Learning 【Models】【Datasets】【Code】【RLVR 训练经验：iou 奖励设计】
[ ] 【2505】【X-Reasoner】X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains 【Code】【文本推理能力泛化到多模态】
[ ] 【2504】【VL-Rethinker : ViRL39K】VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning 【Blog】【Models】【Dataset Code】【①SSR 样本重放解决优势消失】
[ ] 【2504】【Perception-R1】Perception-R1: Pioneering Perception Policy with Reinforcement Learning 【Model】【Datasets】【Code】【① 显式 CoT 不是对所有任务都有效，靠直觉的任务思考多余；② 合适的奖励设计带来健康的学习曲线 ③ 困惑度是 RL 对 SFT 的优势】
[ ] 【2504】【SOTA with Less : ThinkLite-VL】SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement 【Model】【Datasets】【Code】【课程学习： ① 蒙特卡洛树搜索衡量难度；② 构建有挑战性的样本】
[ ] 【2504】【VLM-R1】VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model 【Model】【Dataset】【Demo】【Code】【强化视觉任务】
[ ] 【2504】【CrowdVLM-R1】CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward 【Dataset】【Code】【RLVR 训练经验：FGRPR 精细的 Reward 设计】
[ ] 【2503】【Reason-RFT】Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning 【Blog】【Dataset】【Code】【RLVR 训练经验：训练范式】
[ ] 【2503】【OpenVLThinker】OpenVLThinker: An Early Exploration to Vision-Language Reasoning via Iterative Self-Improvement 【Model】【Code】【Datasets】【RLVR 训练经验：早期数据合成】
[ ] 【2503】【OThink-MR1】OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning【RLVR 训练经验：没有开源】
[ ] 【2503】【R1-Onevision】R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization 【Model】【Dataset】【Demo】【Code】【RLVR 训练经验：图像形式化文本表示】
[X] 【2503】【LMM-R1：VerMulti】LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL 【Code】【Datasets】【Blog】【RLVR 训练经验：两阶段 ① 基础推理增强 ② 多模态泛化训练】
[ ] 【2503】【Curr-ReFT】Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning 【Models】【Dataset】【Code】【RLVR 训练经验：课程强化微调（Curr-ReFT）解决 OOD】
[ ] 【2503】【VisualThinker-R1-Zero】R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model 【Code】【RLVR 训练经验：早期复现经验】
[X] 【2503】【Vision-R1】Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models 【Code】【Blog】【Datasets-Cold-Start】【Datasets-RLVR】【① 构造冷启动数据 ②渐进式思维抑制训练（PTST）策略】
[ ] 【2503】【Visual-RFT】Visual-RFT: Visual Reinforcement Fine-Tuning 【Blog】【Datasets】【Code】【IOU 等奖励设计】
[X] 【2503】【MM-Eureka：MMK12】MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning 【Models】【Datasets-MMK12】【Dataset-MM-Eureka】【Code】【高质量多模态数学推理数据集】
[ ] 【2502】【R1-V 项目训练经验 Blog】RLVR in Vision Language Models: Findings, Questions and Directions
[ ] 【2502】【Task Galaxy】TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types【Code】【文档】【数据集标注 pipeline】
[ ] 【2501】【Virgo】Virgo: A Preliminary Exploration on Reproducing o1-like MLLM 【Model】【Code】【文本推理能力可以跨模态迁移】
[ ] 【2412】【Datasets】【Mulberry】Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search 【Model】【Code】【集成学习多个模型协蒙特卡洛搜索树构建Mulberry-260k】
AutoThink
[ ] 【2509】【MARS2】Multimodal Reasoning and Slow Thinking in the Large Model Era: Towards System 2 and Beyond
[ ] 【2508】【R4B】R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning
[X] 【2508】【deep think with confidence】Deep Think with Confidence
[ ] 【2506】【LC-R1】Optimizing Length Compression in Large Reasoning Models【Abstract】【Code】【精准减少 think 长度】
[ ] 【2506】【SRPO】SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning 【Blog】【Dataset】【Code】【奖励反思 think】
[ ] 【2505】【Qwen3 的 AutoThink】Qwen3-RL训练过程详解
[ ] 【2505】【PixelThink】PixelThink: Towards Efficient Chain-of-Pixel Reasoning 【Blog】【Code】【AutoThink：压缩 token】
[ ] 【2505】【MLRM-Halu】More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models 【Blog】【Benchmark】【Code】【解决长推理容易产生幻觉的问题】
[ ] 【2505】【Learning When to Think】Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL【Abstract】【Code】【Datasets】【自适应切换长短思考】
[ ] 【2505】【Thinkless】Thinkless: LLM Learns When to Think【Code】【Models】【Datasets-Warmup】【Datasets-RL】【AutoThink，自适应思考】
[ ] 【2505】【小模型推理效率】Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement【AutoThink】
[ ] 【2505】【ToN : Think Efficiency Information-Theoretic】Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens【AutoThink】
[X] 【2505】【ToN math sft】Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models 【Models】【Datasets】【Code】【AutoThink】
[ ] 【2504】【FAST】Fast-Slow Thinking for Large Vision-Language Model Reasoning 【Code】【AutoThink】
[ ] 【2504】【VLAA-Thinking】SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models【Models】【Dataset】【Code】【Blog】【AutoThink：伪推理，有效思考】
[ ] 【2503】【Think or Not Think】Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning 【Models】【Datasets】【Code】【AutoThink：视觉感知任务不需要 CoT】
GRPO 改进
[ ] 【2510】【综述】PPO,GRPO,DPO,ARPO算法及其 40+变种｜HF Papers 论文盘点
[ ] 【2510】【ASPO】ASPO: Asymmetric Importance Sampling Policy Optimization【Abstract】
[ ] 【2508】【GFPO 微软】Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning
[ ] 【2508】【GSPO】Group Sequence Policy Optimization【Abstract】
[ ] 【2507】【GMPO】Geometric-Mean Policy Optimization【Abstract】
[ ] 【2507】【GHPO】GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning【奖励稀疏，学习停滞，课程学习，math 提升】
[ ] 【2507】【QPRO】Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions【Abstract】【相对百分比奖励】
[ ] 【2506】【SRPO】SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning 【Blog】【Dataset】【Code】【奖励反思 think】
[X] 【2505】【GiGPO】Group-in-Group Policy Optimization for LLM Agent Training
[X] 【2505】【A*-PO】Accelerating RL for LLM Reasoning with Optimal Advantage Regression
[ ] 【2505】【NFT 负意识微调】Bridging Supervised Learning and Reinforcement Learning in Math Reasoning【实现了 NFT 负监督微调等价于 GRPO】
[ ] 【2505】【R1-ShareVL】R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO 【Code】【Models】【Datasets】【ShareGRPO缓解RL中稀疏奖励和优势消失问题，扩展问题空间，共享问题轨迹】
[ ] 【2505】【TrustGRPO / SophiaVL-R1】SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward 【Models】【Datasets】【Code】【强化学习过程奖励】
[ ] 【2505】【DPO vs GRPO】Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO
[ ] 【2504】【VAPO】VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks
[ ] 【2503】【DrGRPO】Understanding R1-Zero-Like Training: A Critical Perspective【Abstract】
[ ] 【2503】【DAPO】DAPO: An Open-Source LLM Reinforcement Learning System at Scale【Blog】【Code】【Datasets】【Model】
[ ] 【2503】【stepGRPO / R1-VL】R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization 【Model】【Code】【步骤 Reward】
[X] 【2502】【SPO】Self-Supervised Prompt Optimization
未归类
[ ] 【2506】【ViCrit】ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs 【Models】【Datasets】【Code】【解决幻觉】
[ ] 【2506】【Vision Matters】Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning 【Model】【Datasets】【Code】【视觉扰动增强视觉模态】
[ ] 【2506】【ViGaL】Play to Generalize: Learning to Reason Through Game Play 【Blog】【Model】【Code】【游戏训练迁移到推理能力】
[ ] 【2506】【RAP】Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning 【Code】
[ ] 【2506】【RACRO】Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning 【Models】【Demo】【Code】
[ ] 【2506】【Rex-Thinker】Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning 【Blog】【Model】【Dataset】【Demo】【Code】【grounding 推理可解释性】
[ ] 【2505】【DINO-R1】DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models 【Blog】【Code】【对 DINO 这种传统视觉模型做上下文强化学习】
[ ] 【2505】【ViGoRL】Grounded Reinforcement Learning for Visual Reasoning 【Blog】【Code】【grounding 推理】
[ ] 【2505】【Jigsaw-R1】Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles 【Datasets】【Code】
[ ] 【2505】【VRAG-RL】VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning 【Models】【Code】
[ ] 【2505】【SATORI-R1】SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards 【Model】【Dataset】【Code】
[X] 【2505】【URSA】URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics 【Model】【Datasets】【Code】【指标真实性存疑？高质量 math 数据集合成】
[ ] 【2505】【GRE Suite】GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains 【Code】【地理定位推理】
[ ] 【2505】【STAR-R1】STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs 【Code】【空间推理能力】
[ ] 【2505】【VARD (generation)】VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL
[ ] 【2505】【Visionary-R1】Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning 【Blog】【Code】【模态平衡：推理前先解释图像】
[ ] 【2505】【VisualQuality-R1】VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank 【Models】【Code】【强化图像质量评估任务】
[ ] 【2505】【UniVG-R1】UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning 【Blog】【Model】【Dataset】【Code】【grounding 推理】
[ ] 【2505】【G1】G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning 【Code】【VLM-Gym 环境，强化视觉游戏】
[ ] 【2505】【VisionReasoner】VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning 【Model】【Dataset】【Code】【框架把多种视觉推理任务结构化规范化处理】
[ ] 【2505】【GuardReasoner-VL】GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning 【Code】【Datasets-JSON】【Datasets-Images】【Models】【安全审核推理】
[ ] 【2504】【Relation-R1】Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension 【Code】【多实体之间视觉语义结构化】
[ ] 【2504】【R1-SGG】Compile Scene Graphs with Reinforcement Learning 【Code】【视觉表征结构化提取】
[ ] 【2504】【NoisyRollout】Reinforcing Visual Reasoning with Data Augmentation 【Models】【Datasets】【Code】【视觉干扰，数据增强】
[ ] 【2504】【Qwen-AD】Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning 【Code】【假设推理，主动推理】
[ ] 【2504】【SimpleAR (generation)】SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL 【Models】【Code】【自回归生成架构】
[ ] 【2504】【MAYE】Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme 【Dataset】【Code】【简化的 RL 训练框架和评估，过程动态可解释】
[ ] 【2503】【Q-Insight】Q-Insight: Understanding Image Quality via Visual Reinforcement Learning 【Code】【Model】【图像质量评估】
[ ] 【2503】【Seg-Zero】Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement 【Model】【Dataset】【Code】【RL 训练思维链增强图像分割效果】
[ ] 【2502】【WebLI-100B】Scaling Pre-training to One Hundred Billion Data for Vision Language Models【Abstract】【100B 的数据集】
[X] 【2501】【MCQAutoConverter】Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation【Blog】【Model】【Code】【open ended 转化为 mcq】
[ ] 【2501】【Text-to-image COT】Can We Generate Images with CoT? Let’s Verify and Reinforce Image Generation Step by Step 【Blog】【Datasets】【Code】【图像生成：验证 CoT】
[ ] 【2411】【Insight-V】Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models 【Model】【Code】【古早的 CoT 思路】
模态融合 & 表征
图文交错思维链
[ ] 【2510】【MathCanvas】MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning【① 大语言模型执行内在的 VCoT 推理解决数学问题 ② 构建了两个大规模语料库，用于两阶段训练】
[ ] 【2507】【Zebra-CoT】Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning【图像思考：过程有效的视觉文本交替的数据集】
视觉表征
[ ] 【2307】【Meta-Transformer】Meta-Transformer: A Unified Framework for Multimodal Learning
[ ] 【2307】【UnIVAL】UnIVAL: Unified Model for Image, Video, Audio and Language Tasks
原生多模态融合
[ ] 【2510】【From Pixels to Words】From Pixels to Words -- Towards Native Vision-Language Primitives at Scale
[ ] 【2510】【SAIL-embedding】SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model
[ ] 【2504】【SAIL】The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer【Abstract】
Latent Reasoning
【论文归纳整理】Latent Space Reasoning
latent reasoning用来推理是误入歧途了
Awesome-Latent-Reasoning
Awesome-Latent-CoT
stable-latent-reasoning
[ ] 【2510】【Patch-as-Decodable-Token】【颜水成】Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs【Abstract】
[ ] 【2510】【SwiReasoning】SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs
[ ] 【2509】【Latent Visual Reasoning】Latent Visual Reasoning【视觉 logits 参与输出和思维】
[ ] 【2507】【综述】A Survey on Latent Reasoning【Abstract】【Code】
[ ] 【2507】【综述】Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning【Awesome-Latent-CoT】
[ ] 【2506】【No Free Lunch】No Free Lunch: Rethinking Internal Feedback for LLM Reasoning【内部熵奖励】
[ ] 【2505】【Think Silently, Think Fast】Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains【latent token 压缩思维链长度】
[ ] 【2505】【Soft Thinking】Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space【Code】
[ ] 【2505】【HRPO】Hybrid Latent Reasoning via Reinforcement Learning
[ ] 【2505】From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning【Yann LeCun】
[ ] 【2505】Reinforced Latent Reasoning for LLM-based Recommendation
[ ] 【2504】Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models
[ ] 【2503】Reasoning to Learn from Latent Thoughts
[ ] 【2503】Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation
[ ] 【2502】Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach【Abstract/Abstract2】【模仿脑内循环】
[ ] 【2502】Reasoning with Latent Thoughts: On the Power of Looped Transformers
[ ] 【2502】Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation
[ ] 【2412】【CoConut】Training Large Language Models to Reason in a Continuous Latent Space【Abstract】
[ ] 【2412】【Byte Latent Transformer】【Meta】Byte Latent Transformer: Patches Scale Better Than Tokens【Abstract】
[ ] 【2411】Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding
[ ] 【2411】Interleaved-Modal Chain-of-Thought【Abstract】
[ ] 【2411】Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?【ACL】
[ ] 【2312】LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning
图像思考
[ ] 【2509】【Mini-o3】Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search【Code】
[ ] 【2505】【知乎】视觉推理四篇：DeepEyes、TwGI、PixelReasoner、VisualPlanner
[ ] 【2505】【Active-O3】Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO 【Blog】【Model】【Code】【图像思考：主动视觉感知】
[ ] 【2505】【v1】v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning 【Model】【Code】【图像思考： gpt-o3 图片多次访问】
[ ] 【2505】【VLM-R^3】VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought 【图像思考】
[ ] 【2505】【Pixel Reasoner】Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning 【Blog】【Models】【Datasets】【Demo】【Code】【图像思考】
[ ] 【2505】【GRIT】GRIT: Teaching MLLMs to Think with Images 【Blog】【Demo】【Code】【Datasets】【图像思考：模态交互，grounding】
[ ] 【2505】【Chain-of-Focus】Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL 【Blog】【Datasets】【Models】【Code】【图像思考；自适应缩放】
[ ] 【2505】【DeepEyes】Incentivizing "Thinking with Images" via Reinforcement Learning 【Blog】【Model】【Dataset】【Code】【图像思考】
[ ] 【2505】【Visual-ARFT】Visual Agentic Reinforcement Fine-Tuning 【Models】【Datasets】【Code】【图像思考：agent】
[ ] 【2505】【VPRL：Visual Planning】Visual Planning: Let’s Think Only with Images 【Code】【图像思考：纯视觉规划，独立于文本】
[ ] 【2505】【OpenThinkIMG】OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning 【Model】【Datasets】【Code】【图像思考：Agent，V-ToolRL】
单模态虚假关联
[ ] 【2508】【LookBack】Look-Back: Implicit Visual Re-focusing in MLLM Reasoning【Abstract】
[ ] 【2503】【小红书：奖励模型虚假关联】The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models【Abstract】
Video（视频）
空间理解
[X] 【2507】【MindJourney】【NIPS25】MindJourney: Test-Time Scaling with World Models for Spatial Reasoning 【blog】
  ○ 世界模型+vlm促进空间推理 认知心理学中核心是构建认知地图，world model相当于直接想象画面，更直观，例如SpaceR则相当于文字形式，属于两种不同道路
[ ] 【2505】【SpaceR】【NIPS25】SpaceR: Reinforcing MLLMs in Video Spatial Reasoning 【Model】【Dataset】【Code】
  ○ 比较全面 包含grpo类强化学习 认知地图构建（文字版，json结构形式），完整开源数据等
[ ] 【2506】【OmniSpatial】【NIPS25】OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models 【blog】【Dataset】【Code】
  ○ 主要做bench 分了很多类别 没太大用 数据构建可参考
[ ] 【2506】【MindCube】Spatial Mental Modeling from Limited Views 【blog】
  ○ 李飞飞组 主要详细讨论认知地图的构建 感觉不如world model方法
[ ] 【2412】【VSIbench】Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces 【blog】
  ○ 李飞飞组 空间理解奠基性文章之一 vsibench基本属于空间理解必备实验 不过基本基于scannet（第一人称 室内）比较局限 且问题范式不够多样化 估计是暂时性的bench
利用VGGT: Visual Geometry Grounded Transformer作为空间encoder：
[X] 【2505】【NIPS25】【VG-LLM】Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors 【Model】【Dataset】【Code】
  ○ 最中规中矩的做法 vggt输出特征直接投影到vision encoder同维度，暴力相加或concat，无RL，实测效果比不上原模型
[X] 【2505】【NIPS25】【Spatial-MLLM】Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence 【blog】
  ○ 模型架构和vgllm类似（额外添加一些抽帧策略），但数据集根据scannet自行构建，且包含cold start加grpo
[ ] 【2505】【VLM-3R】VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction 【blog】
  ○ 类似上面俩 但encoder不是mlp 用的q-former来嵌入到llm 以及提出vstibench（时空类型mcq，vsibench此类时-空的数据较少）

Video RL
[ ] 【2510】【综述】Video-LMM后训练综述：GRPO强化学习驱动视频推理新范式
细粒度视频理解
流式文本/视频理解
视频流
[ ] 【2510】【RTFM】RTFM: A Real-Time Frame Model【实时帧世界模型】
[ ] 【2510】【StreamingVLM】StreamingVLM: Real-Time Understanding for Infinite Video Streams
[ ] 【2509】【StreamForest】StreamForest: Efficient Online Video Understanding with Persistent Event Memory【Abstract】
[ ] 【2504】【Streaming Multitask】【ICCV25】Learning Streaming Video Representation via Multitask Training【上交谢伟迪正在升级的工作】
[ ] 【2504】【LiveCC】【CVPR25】LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale【Blog】
[ ] 【2406】【CVPR24】VideoLLM-online: Online Video Large Language Model for Streaming Video【Code】
文本流
[ ] 【2309】【StreamingLLM】Efficient Streaming Language Models with Attention Sinks【Abstract】【Code】
Chain of Frames
[ ] 【2510】【Abstract】【CoF 如何让帧间逻辑从「隐式对齐」变成「显式思考」】
未归类
[X] 【2508】【SLFG】Enhancing Long Video Question Answering with Scene-Localized Frame Grouping 【Blog】【Dataset】【Benchmark 论文】
[ ] 【2507】【LLaVA-ST】LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding 【Code】【Model】【Dataset】【Benchmark】
[ ] 【2507】【Time-R1】Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding 【Code】【Model】【Dataset】【Blog】
[ ] 【2507】【VideoExpert】VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding
[ ] 【2506】【DeepVideo-R1】DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO 【Code】
[ ] 【2506】【ReFoCUS】ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding
[ ] 【2506】【video-SALMONN 2】video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models 【】【Code】【】
[ ] 【2505】【TRACE】TRACE: Temporal Grounding Video LLM via Causal Event Modeling【Code】【Model】【Dataset】
[ ] 【2505】【TW-GRPO】Reinforcing Video Reasoning with Focused Thinking 【Model】【Code】
[ ] 【2505】【Spatial-MLLM】Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence 【Blog】【Model】【Code】
[ ] 【2505】【VAU-R1】VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning 【Blog】【Dataset】【Code】
[ ] 【2505】【MUSEG】MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding 【Models】【Code】
[ ] 【2505】【VerIPO】VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization 【Model】【Code】
[X] 【2504】【slow fast LLaVA】【keye1.5 使用了】【Abstract】
[ ] 【2504】【TinyLLaVA-Video-R1】TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning 【Model】【Code】
[ ] 【2504】【VideoChat-R1】VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning 【Model】【Code】
[ ] 【2504】【Spatial-R1】Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning 【Code】
[ ] 【2504】【R1-Zero-VSI】Improved Visual-Spatial Reasoning via R1-Zero-Like Training 【Code】
[ ] 【2503】【SEED-Bench-R1】Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 【Dataset】【Code】
[ ] 【2503】【Video-R1】Video-R1: Reinforcing Video Reasoning in MLLMs 【Model】【Dataset】【Code】【视频推理，OCR 奖励】
[ ] 【2503】【TimeZero】TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM 【Code】【Model】【视频时序定位】
[ ] 【2401】【TimeSuite】TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning【Code】【Model】【Dataset】
[ ] 【2411】【Seq2Time】Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding
